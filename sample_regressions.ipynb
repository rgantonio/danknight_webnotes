{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sample_regressions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3PC2S2PJY6SFFHpQ832dH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgantonio/danknight_webnotes/blob/main/sample_regressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Regression Routines\n",
        "The guide is simple and used for the sake of simple regressions routine. The steps are:\n",
        "\n",
        "1. Data clean the data set\n",
        "- Preferrably there are no NaN\n",
        "- All empty sets are filled with entries\n",
        "- All categorical sets are encoded to numerical values (make sure they make sense)\n",
        "2. Seperate the $X$ features and $y$ data.\n",
        "3. Split the data into train and test sets.\n",
        "4. Scale the data to appropriate \n",
        "- Normalize the features and scale them into standard form\n",
        "5. Initialize (or import) the model\n",
        "- Ridge\n",
        "- Regression\n",
        "- Lasso\n",
        "- Combinations with CV or so\n",
        "6. Create the model\n",
        "7. Fit the model. Can fit with cross validation models.\n",
        "8. Analyze scores so and tune hyperparameters\n",
        "- This includes reporting mean squared errors and so on\n",
        "9. Repeat 6-7 until we get the desired scores\n",
        "10. Save the model\n",
        "\n"
      ],
      "metadata": {
        "id": "qcxHZX3LyWSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WARNING** The codes below won't work. It's just a sample guide. Specifically it will use the cross_validate format because it provides a rigorous analysis for the error metrics."
      ],
      "metadata": {
        "id": "FQpp_DBe50mF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "# Importing important packages\n",
        "##########################################\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "##########################################\n",
        "# 1. Let's assume that a data set has been cleaned. \n",
        "#    Let's import the data in this part.\n",
        "##########################################\n",
        "\n",
        "df = pd.read_csv(\"cool_data.csv\")\n",
        "\n",
        "# Make it a habit to inspect the first few entries\n",
        "df.head()\n",
        "\n",
        "# Make it a habit to inspect the overall information\n",
        "df. info()\n",
        "\n",
        "##########################################\n",
        "# 2. Separate X and y data\n",
        "##########################################\n",
        "X = df.drop('y_data',axis = 1)\n",
        "y = df['y_data']\n",
        "\n",
        "##########################################\n",
        "# 3. Split data into train and test sets\n",
        "##########################################\n",
        "\n",
        "# First import the train_test_split package\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the train and test sets. Make sure to indicate the X features and y labels\n",
        "# test_size is the percentage of tests from the entire data set\n",
        "# random_state indicates the random seed\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "##########################################\n",
        "# 4. Scale the data into standard distribution\n",
        "##########################################\n",
        "\n",
        "# Import the StandardScaler package\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Declare the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Compute the mean and standard deviation and save it into the scaler\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform the train and test features according to the precomputed fitting parameters\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "##########################################\n",
        "# 5. Initialize or import the model\n",
        "##########################################\n",
        "\n",
        "# Importing ridge regression (this is the L2 version)\n",
        "# Note you can take in Lasso or ElasticNet depending on what you need\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "##########################################\n",
        "# 6. Creating the model\n",
        "##########################################\n",
        "# The alpha term here is the learning rate / parameter\n",
        "model = Ridge(alpha=100)\n",
        "\n",
        "##########################################\n",
        "# 7. Fit the model\n",
        "##########################################\n",
        "\n",
        "# In some cases we would like to import the models with cross validation scoring\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# We fit the model at the same time generate the scoring metrics for us to tune the hyperparameters\n",
        "# We have the model previous initialized\n",
        "# We have X_train data, y_train data\n",
        "# We set the scoring metrics\n",
        "# Do a K-fold of 5 from the cv =  argument\n",
        "scores = cross_validate(model,X_train,y_train,\n",
        "                         scoring=['neg_mean_absolute_error','neg_mean_squared_error','max_error'],cv=5)\n",
        "\n",
        "##########################################\n",
        "# 8. Analyze scores so and tune hyperparameters\n",
        "##########################################\n",
        "\n",
        "# If you either used cross_validate or cross_val_score you can just call scores to view the scores\n",
        "# If not, you can import:\n",
        "#   from sklearn.metrics import mean_squared_error\n",
        "# Then compare the mean squared error by:\n",
        "# mean_squared_error(y_test,y_test_pred)\n",
        "# Of course don't forget to create the predictions for this part\n",
        "\n",
        "##########################################\n",
        "# 9. Repeat 6-8 depending on how you tune the hyperparameters\n",
        "##########################################\n",
        "\n",
        "# At this point you should also check the data depending on your hold out test \n",
        "# At this point this is the final reporting of your data\n",
        "model = Ridge(alpha=1)\n",
        "model.fit(X_train,y_train)\n",
        "y_final_test_pred = model.predict(X_test)\n",
        "mean_squared_error(y_test,y_final_test_pred)\n",
        "\n",
        "##########################################\n",
        "# 10. Save the model!\n",
        "##########################################\n",
        "\n",
        "# Import the joblib to save the model\n",
        "from joblib import dump, load\n",
        "\n",
        "# Use dump to save the model\n",
        "dump(final_model, 'sales_model.joblib') \n",
        "\n",
        "# Use load to import the model\n",
        "loaded_model = load('sales_model.joblib')\n",
        "\n",
        "# Run the imported model\n",
        "loaded_model.predict(campaign)"
      ],
      "metadata": {
        "id": "LRTy-DMS5jps"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}